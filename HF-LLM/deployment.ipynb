{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHux0Xbm_Uet"
      },
      "outputs": [],
      "source": [
        "TGI is easy to install and use, with deep integration into the Hugging Face ecosystem.\n",
        "\n",
        "First, launch the TGI server using Docker:\n",
        "\n",
        "```sh\n",
        "docker run --gpus all \\\n",
        "    --shm-size 1g \\\n",
        "    -p 8080:80 \\\n",
        "    -v ~/.cache/huggingface:/data \\\n",
        "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
        "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct\n",
        "```\n",
        "\n",
        "Then interact with it using Hugging Face's InferenceClient:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize client pointing to TGI endpoint\n",
        "client = InferenceClient(\n",
        "    model=\"http://localhost:8080\",  # URL to the TGI server\n",
        ")\n",
        "\n",
        "# Text generation\n",
        "response = client.text_generation(\n",
        "    \"Tell me a story\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    details=True,\n",
        "    stop_sequences=[],\n",
        ")\n",
        "print(response.generated_text)\n",
        "\n",
        "# For chat format\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Alternatively, you can use the OpenAI client:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize client pointing to TGI endpoint\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8080/v1\",  # Make sure to include /v1\n",
        "    api_key=\"not-needed\",  # TGI doesn't require an API key by default\n",
        ")\n",
        "\n",
        "# Chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "llama.cpp is easy to install and use, requiring minimal dependencies and supporting both CPU and GPU inference.\n",
        "\n",
        "First, install and build llama.cpp:\n",
        "\n",
        "```sh\n",
        "# Clone the repository\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp\n",
        "\n",
        "# Build the project\n",
        "make\n",
        "\n",
        "# Download the SmolLM2-1.7B-Instruct-GGUF model\n",
        "curl -L -O https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct.Q4_K_M.gguf\n",
        "```\n",
        "\n",
        "Then, launch the server (with OpenAI API compatibility):\n",
        "\n",
        "```sh\n",
        "# Start the server\n",
        "./server \\\n",
        "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8080 \\\n",
        "    -c 4096 \\\n",
        "    --n-gpu-layers 0  # Set to a higher number to use GPU\n",
        "```\n",
        "\n",
        "Interact with the server using Hugging Face's InferenceClient:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize client pointing to llama.cpp server\n",
        "client = InferenceClient(\n",
        "    model=\"http://localhost:8080/v1\",  # URL to the llama.cpp server\n",
        "    token=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
        ")\n",
        "\n",
        "# Text generation\n",
        "response = client.text_generation(\n",
        "    \"Tell me a story\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    details=True,\n",
        ")\n",
        "print(response.generated_text)\n",
        "\n",
        "# For chat format\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Alternatively, you can use the OpenAI client:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize client pointing to llama.cpp server\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8080/v1\",\n",
        "    api_key=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
        ")\n",
        "\n",
        "# Chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"smollm2-1.7b-instruct\",  # Model identifier can be anything as server only loads one model\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "vLLM is easy to install and use, with both OpenAI API compatibility and a native Python interface.\n",
        "\n",
        "First, launch the vLLM OpenAI-compatible server:\n",
        "\n",
        "```sh\n",
        "python -m vllm.entrypoints.openai.api_server \\\n",
        "    --model HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8000\n",
        "```\n",
        "\n",
        "Then interact with it using Hugging Face's InferenceClient:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize client pointing to vLLM endpoint\n",
        "client = InferenceClient(\n",
        "    model=\"http://localhost:8000/v1\",  # URL to the vLLM server\n",
        ")\n",
        "\n",
        "# Text generation\n",
        "response = client.text_generation(\n",
        "    \"Tell me a story\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    details=True,\n",
        ")\n",
        "print(response.generated_text)\n",
        "\n",
        "# For chat format\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "Alternatively, you can use the OpenAI client:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize client pointing to vLLM endpoint\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"not-needed\",  # vLLM doesn't require an API key by default\n",
        ")\n",
        "\n",
        "# Chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "### Basic Text Generation\n",
        "\n",
        "Let's look at examples of text generation with the frameworks:\n",
        "\n",
        "<hfoptions id=\"inference-frameworks\" >\n",
        "\n",
        "<hfoption value=\"tgi\" label=\"TGI\">\n",
        "\n",
        "First, deploy TGI with advanced parameters:\n",
        "```sh\n",
        "docker run --gpus all \\\n",
        "    --shm-size 1g \\\n",
        "    -p 8080:80 \\\n",
        "    -v ~/.cache/huggingface:/data \\\n",
        "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
        "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
        "    --max-total-tokens 4096 \\\n",
        "    --max-input-length 3072 \\\n",
        "    --max-batch-total-tokens 8192 \\\n",
        "    --waiting-served-ratio 1.2\n",
        "```\n",
        "\n",
        "Use the InferenceClient for flexible text generation:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(model=\"http://localhost:8080\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=200,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Raw text generation\n",
        "response = client.text_generation(\n",
        "    \"Write a creative story about space exploration\",\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    do_sample=True,\n",
        "    details=True,\n",
        ")\n",
        "print(response.generated_text)\n",
        "```\n",
        "\n",
        "Or use the OpenAI client:\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "For llama.cpp, you can set advanced parameters when launching the server:\n",
        "\n",
        "```sh\n",
        "./server \\\n",
        "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8080 \\\n",
        "    -c 4096 \\            # Context size\n",
        "    --threads 8 \\        # CPU threads to use\n",
        "    --batch-size 512 \\   # Batch size for prompt evaluation\n",
        "    --n-gpu-layers 0     # GPU layers (0 = CPU only)\n",
        "```\n",
        "\n",
        "Use the InferenceClient:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(model=\"http://localhost:8080/v1\", token=\"sk-no-key-required\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=200,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# For direct text generation\n",
        "response = client.text_generation(\n",
        "    \"Write a creative story about space exploration\",\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    details=True,\n",
        ")\n",
        "print(response.generated_text)\n",
        "```\n",
        "\n",
        "Or use the OpenAI client for generation with control over the sampling parameters:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat.completions.create(\n",
        "    model=\"smollm2-1.7b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        "    top_p=0.95,  # Nucleus sampling probability\n",
        "    frequency_penalty=0.5,  # Reduce repetition of frequent tokens\n",
        "    presence_penalty=0.5,  # Reduce repetition by penalizing tokens already present\n",
        "    max_tokens=200,  # Maximum generation length\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "You can also use llama.cpp's native library for even more control:\n",
        "\n",
        "```python\n",
        "# Using llama-cpp-python package for direct model access\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "llm = Llama(\n",
        "    model_path=\"smollm2-1.7b-instruct.Q4_K_M.gguf\",\n",
        "    n_ctx=4096,  # Context window size\n",
        "    n_threads=8,  # CPU threads\n",
        "    n_gpu_layers=0,  # GPU layers (0 = CPU only)\n",
        ")\n",
        "\n",
        "# Format prompt according to the model's expected format\n",
        "prompt = \"\"\"<|im_start|>system\n",
        "You are a creative storyteller.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Write a creative story\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Generate response with precise parameter control\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    frequency_penalty=0.5,\n",
        "    presence_penalty=0.5,\n",
        "    stop=[\"<|im_end|>\"],\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "For advanced usage with vLLM, you can use the InferenceClient:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(model=\"http://localhost:8000/v1\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=200,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# For direct text generation\n",
        "response = client.text_generation(\n",
        "    \"Write a creative story about space exploration\",\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    details=True,\n",
        ")\n",
        "print(response.generated_text)\n",
        "```\n",
        "\n",
        "You can also use the OpenAI client:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"not-needed\")\n",
        "\n",
        "# Advanced parameters example\n",
        "response = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=200,\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "vLLM also provides a native Python interface with fine-grained control:\n",
        "\n",
        "```python\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Initialize the model with advanced parameters\n",
        "llm = LLM(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    gpu_memory_utilization=0.85,\n",
        "    max_num_batched_tokens=8192,\n",
        "    max_num_seqs=256,\n",
        "    block_size=16,\n",
        ")\n",
        "\n",
        "# Configure sampling parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        "    top_p=0.95,  # Consider top 95% probability mass\n",
        "    max_tokens=100,  # Maximum length\n",
        "    presence_penalty=1.1,  # Reduce repetition\n",
        "    frequency_penalty=1.1,  # Reduce repetition\n",
        "    stop=[\"\\n\\n\", \"###\"],  # Stop sequences\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Write a creative story\"\n",
        "outputs = llm.generate(prompt, sampling_params)\n",
        "print(outputs[0].outputs[0].text)\n",
        "\n",
        "# For chat-style interactions\n",
        "chat_prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
        "]\n",
        "formatted_prompt = llm.get_chat_template()(chat_prompt)  # Uses model's chat template\n",
        "outputs = llm.generate(formatted_prompt, sampling_params)\n",
        "print(outputs[0].outputs[0].text)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "## Advanced Generation Control\n",
        "\n",
        "### Token Selection and Sampling\n",
        "\n",
        "The process of generating text involves selecting the next token at each step. This selection process can be controlled through various parameters:\n",
        "\n",
        "1. **Raw Logits**: The initial output probabilities for each token\n",
        "2. **Temperature**: Controls randomness in selection (higher = more creative)\n",
        "3. **Top-p (Nucleus) Sampling**: Filters to top tokens making up X% of probability mass\n",
        "4. **Top-k Filtering**: Limits selection to k most likely tokens\n",
        "\n",
        "Here's how to configure these parameters:\n",
        "\n",
        "<hfoptions id=\"inference-frameworks\" >\n",
        "\n",
        "<hfoption value=\"tgi\" label=\"TGI\">\n",
        "\n",
        "```python\n",
        "client.generate(\n",
        "    \"Write a creative story\",\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        "    top_p=0.95,  # Consider top 95% probability mass\n",
        "    top_k=50,  # Consider top 50 tokens\n",
        "    max_new_tokens=100,  # Maximum length\n",
        "    repetition_penalty=1.1,  # Reduce repetition\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "```python\n",
        "# Via OpenAI API compatibility\n",
        "response = client.completions.create(\n",
        "    model=\"smollm2-1.7b-instruct\",  # Model name (can be any string for llama.cpp server)\n",
        "    prompt=\"Write a creative story\",\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        "    top_p=0.95,  # Consider top 95% probability mass\n",
        "    frequency_penalty=1.1,  # Reduce repetition\n",
        "    presence_penalty=0.1,  # Reduce repetition\n",
        "    max_tokens=100,  # Maximum length\n",
        ")\n",
        "\n",
        "# Via llama-cpp-python direct access\n",
        "output = llm(\n",
        "    \"Write a creative story\",\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    max_tokens=100,\n",
        "    repeat_penalty=1.1,\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "```python\n",
        "params = SamplingParams(\n",
        "    temperature=0.8,  # Higher for more creativity\n",
        "    top_p=0.95,  # Consider top 95% probability mass\n",
        "    top_k=50,  # Consider top 50 tokens\n",
        "    max_tokens=100,  # Maximum length\n",
        "    presence_penalty=0.1,  # Reduce repetition\n",
        ")\n",
        "llm.generate(\"Write a creative story\", sampling_params=params)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "### Controlling Repetition\n",
        "\n",
        "Both frameworks provide ways to prevent repetitive text generation:\n",
        "\n",
        "<hfoptions id=\"inference-frameworks\" >\n",
        "\n",
        "<hfoption value=\"tgi\" label=\"TGI\">\n",
        "\n",
        "```python\n",
        "client.generate(\n",
        "    \"Write a varied text\",\n",
        "    repetition_penalty=1.1,  # Penalize repeated tokens\n",
        "    no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "```python\n",
        "# Via OpenAI API\n",
        "response = client.completions.create(\n",
        "    model=\"smollm2-1.7b-instruct\",\n",
        "    prompt=\"Write a varied text\",\n",
        "    frequency_penalty=1.1,  # Penalize frequent tokens\n",
        "    presence_penalty=0.8,  # Penalize tokens already present\n",
        ")\n",
        "\n",
        "# Via direct library\n",
        "output = llm(\n",
        "    \"Write a varied text\",\n",
        "    repeat_penalty=1.1,  # Penalize repeated tokens\n",
        "    frequency_penalty=0.5,  # Additional frequency penalty\n",
        "    presence_penalty=0.5,  # Additional presence penalty\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "```python\n",
        "params = SamplingParams(\n",
        "    presence_penalty=0.1,  # Penalize token presence\n",
        "    frequency_penalty=0.1,  # Penalize token frequency\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "### Length Control and Stop Sequences\n",
        "\n",
        "You can control generation length and specify when to stop:\n",
        "\n",
        "<hfoptions id=\"inference-frameworks\" >\n",
        "\n",
        "<hfoption value=\"tgi\" label=\"TGI\">\n",
        "\n",
        "```python\n",
        "client.generate(\n",
        "    \"Generate a short paragraph\",\n",
        "    max_new_tokens=100,\n",
        "    min_new_tokens=10,\n",
        "    stop_sequences=[\"\\n\\n\", \"###\"],\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "```python\n",
        "# Via OpenAI API\n",
        "response = client.completions.create(\n",
        "    model=\"smollm2-1.7b-instruct\",\n",
        "    prompt=\"Generate a short paragraph\",\n",
        "    max_tokens=100,\n",
        "    stop=[\"\\n\\n\", \"###\"],\n",
        ")\n",
        "\n",
        "# Via direct library\n",
        "output = llm(\"Generate a short paragraph\", max_tokens=100, stop=[\"\\n\\n\", \"###\"])\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "```python\n",
        "params = SamplingParams(\n",
        "    max_tokens=100,\n",
        "    min_tokens=10,\n",
        "    stop=[\"###\", \"\\n\\n\"],\n",
        "    ignore_eos=False,\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "## Memory Management\n",
        "\n",
        "Both frameworks implement advanced memory management techniques for efficient inference.\n",
        "\n",
        "<hfoptions id=\"inference-frameworks\" >\n",
        "\n",
        "<hfoption value=\"tgi\" label=\"TGI\">\n",
        "\n",
        "TGI uses Flash Attention 2 and continuous batching:\n",
        "\n",
        "```sh\n",
        "# Docker deployment with memory optimization\n",
        "docker run --gpus all -p 8080:80 \\\n",
        "    --shm-size 1g \\\n",
        "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
        "    --model-id HuggingFaceTB/SmolLM2-1.7B-Instruct \\\n",
        "    --max-batch-total-tokens 8192 \\\n",
        "    --max-input-length 4096\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"llama.cpp\" label=\"llama.cpp\">\n",
        "\n",
        "llama.cpp uses quantization and optimized memory layout:\n",
        "\n",
        "```sh\n",
        "# Server with memory optimizations\n",
        "./server \\\n",
        "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8080 \\\n",
        "    -c 2048 \\               # Context size\n",
        "    --threads 4 \\           # CPU threads\n",
        "    --n-gpu-layers 32 \\     # Use more GPU layers for larger models\n",
        "    --mlock \\               # Lock memory to prevent swapping\n",
        "    --cont-batching         # Enable continuous batching\n",
        "```\n",
        "\n",
        "For models too large for your GPU, you can use CPU offloading:\n",
        "\n",
        "```sh\n",
        "./server \\\n",
        "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
        "    --n-gpu-layers 20 \\     # Keep first 20 layers on GPU\n",
        "    --threads 8             # Use more CPU threads for CPU layers\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "<hfoption value=\"vllm\" label=\"vLLM\">\n",
        "\n",
        "vLLM uses PagedAttention for optimal memory management:\n",
        "\n",
        "```python\n",
        "from vllm.engine.arg_utils import AsyncEngineArgs\n",
        "\n",
        "engine_args = AsyncEngineArgs(\n",
        "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
        "    gpu_memory_utilization=0.85,\n",
        "    max_num_batched_tokens=8192,\n",
        "    block_size=16,\n",
        ")\n",
        "\n",
        "llm = LLM(engine_args=engine_args)\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "\n",
        "</hfoptions>\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference)\n",
        "- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)\n",
        "- [vLLM Documentation](https://vllm.readthedocs.io/)\n",
        "- [vLLM GitHub Repository](https://github.com/vllm-project/vllm)\n",
        "- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)\n",
        "- [llama.cpp GitHub Repository](https://github.com/ggerganov/llama.cpp)\n",
        "- [llama-cpp-python Repository](https://github.com/abetlen/llama-cpp-python)\n",
        "\n",
        "\n",
        "<EditOnGithub source=\"https://github.com/huggingface/course/blob/main/chapters/en/chapter2/8.mdx\" />"
      ]
    }
  ]
}